{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624db67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import our modules\n",
    "from src.basic_operations import *\n",
    "from src.filters import *\n",
    "from src.edge_detection import *\n",
    "from src.segmentation import *\n",
    "from src.morphology import *\n",
    "from src.frequency_domain import *\n",
    "from src.feature_detection import *\n",
    "from src.deep_learning import *\n",
    "from src.utils import *\n",
    "\n",
    "# Configure matplotlib\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc6d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to display images\n",
    "def show_comparison(original, processed, titles=['Original', 'Processed']):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    for ax, img, title in zip(axes, [original, processed], titles):\n",
    "        if len(img.shape) == 3:\n",
    "            ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(title, fontsize=14)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_multiple(images, titles, cols=3):\n",
    "    n = len(images)\n",
    "    rows = (n + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 5*rows))\n",
    "    axes = np.array(axes).flatten()\n",
    "    \n",
    "    for i, (ax, img, title) in enumerate(zip(axes, images, titles)):\n",
    "        if len(img.shape) == 3:\n",
    "            ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    for ax in axes[n:]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image\n",
    "# You can replace this with your own image path\n",
    "try:\n",
    "    img = load_image('../test_image/lena.pgm', grayscale=False)\n",
    "except:\n",
    "    # Create a sample image if no test image available\n",
    "    img = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)\n",
    "    # Add some patterns\n",
    "    cv2.circle(img, (128, 128), 50, (255, 0, 0), -1)\n",
    "    cv2.rectangle(img, (50, 50), (100, 100), (0, 255, 0), -1)\n",
    "\n",
    "gray = convert_to_grayscale(img)\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Image info: {get_image_info(img)}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "if len(img.shape) == 3:\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "else:\n",
    "    plt.imshow(img, cmap='gray')\n",
    "plt.title('Sample Image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a8f3e7",
   "metadata": {},
   "source": [
    "## 1. Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac38645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize operations\n",
    "resized_half = resize_image(img, scale=0.5)\n",
    "resized_double = resize_image(img, scale=2.0)\n",
    "print(f\"Original: {img.shape}, Half: {resized_half.shape}, Double: {resized_double.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf8286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotation and flipping\n",
    "rotated = rotate_image(img, 45)\n",
    "flipped_h = flip_image(img, 'horizontal')\n",
    "flipped_v = flip_image(img, 'vertical')\n",
    "\n",
    "show_multiple([img, rotated, flipped_h, flipped_v],\n",
    "              ['Original', 'Rotated 45°', 'Horizontal Flip', 'Vertical Flip'], cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735d0242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brightness and contrast\n",
    "bright = adjust_brightness(img, 50)\n",
    "dark = adjust_brightness(img, -50)\n",
    "high_contrast = adjust_contrast(img, 1.5)\n",
    "low_contrast = adjust_contrast(img, 0.5)\n",
    "\n",
    "show_multiple([img, bright, dark, high_contrast, low_contrast],\n",
    "              ['Original', 'Brighter', 'Darker', 'High Contrast', 'Low Contrast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbad286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma correction\n",
    "gamma_low = gamma_correction(img, 0.5)  # Brightens\n",
    "gamma_high = gamma_correction(img, 2.0)  # Darkens\n",
    "\n",
    "show_multiple([img, gamma_low, gamma_high],\n",
    "              ['Original', 'Gamma=0.5 (Lighter)', 'Gamma=2.0 (Darker)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f8fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram equalization\n",
    "equalized = histogram_equalization(img)\n",
    "clahe = histogram_equalization(img, use_clahe=True)\n",
    "\n",
    "show_multiple([img, equalized, clahe],\n",
    "              ['Original', 'Histogram Equalized', 'CLAHE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e50730",
   "metadata": {},
   "source": [
    "## 2. Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d025f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing filters\n",
    "avg = average_filter(img, kernel_size=5)\n",
    "gauss = gaussian_filter(img, kernel_size=5)\n",
    "med = median_filter(img, kernel_size=5)\n",
    "bilat = bilateral_filter(img)\n",
    "\n",
    "show_multiple([img, avg, gauss, med, bilat],\n",
    "              ['Original', 'Average', 'Gaussian', 'Median', 'Bilateral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eadc50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sharpening and emboss\n",
    "sharp = sharpen_filter(img)\n",
    "unsharp = unsharp_mask(img, amount=1.5)\n",
    "emboss = emboss_filter(img)\n",
    "\n",
    "show_multiple([img, sharp, unsharp, emboss],\n",
    "              ['Original', 'Sharpened', 'Unsharp Mask', 'Emboss'], cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3acab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise addition and removal\n",
    "noisy_gauss = add_gaussian_noise(img, std=30)\n",
    "noisy_sp = add_salt_pepper_noise(img, salt_prob=0.02, pepper_prob=0.02)\n",
    "denoised = non_local_means_denoise(noisy_gauss)\n",
    "\n",
    "show_multiple([img, noisy_gauss, noisy_sp, denoised],\n",
    "              ['Original', 'Gaussian Noise', 'Salt & Pepper', 'Denoised'], cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c021a483",
   "metadata": {},
   "source": [
    "## 3. Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various edge detection methods\n",
    "sobel = sobel_edge_detection(gray)\n",
    "canny = canny_edge_detection(gray, 50, 150)\n",
    "laplacian = laplacian_edge_detection(gray)\n",
    "prewitt = prewitt_edge_detection(gray)\n",
    "roberts = roberts_edge_detection(gray)\n",
    "scharr = scharr_edge_detection(gray)\n",
    "\n",
    "show_multiple([gray, sobel, canny, laplacian, prewitt, roberts],\n",
    "              ['Original', 'Sobel', 'Canny', 'Laplacian', 'Prewitt', 'Roberts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1391daf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto Canny with different sigma values\n",
    "auto1 = auto_canny(gray, sigma=0.2)\n",
    "auto2 = auto_canny(gray, sigma=0.33)\n",
    "auto3 = auto_canny(gray, sigma=0.5)\n",
    "\n",
    "show_multiple([gray, auto1, auto2, auto3],\n",
    "              ['Original', 'Auto Canny σ=0.2', 'Auto Canny σ=0.33', 'Auto Canny σ=0.5'], cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2199d",
   "metadata": {},
   "source": [
    "## 4. Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholding methods\n",
    "simple = simple_threshold(gray, 127)\n",
    "otsu, thresh_val = otsu_threshold(gray)\n",
    "adaptive = adaptive_threshold(gray)\n",
    "\n",
    "print(f\"Otsu optimal threshold: {thresh_val}\")\n",
    "show_multiple([gray, simple, otsu, adaptive],\n",
    "              ['Original', 'Simple (127)', f'Otsu ({thresh_val})', 'Adaptive'], cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8970d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means segmentation\n",
    "kmeans_2 = kmeans_segmentation(img, k=2)\n",
    "kmeans_4 = kmeans_segmentation(img, k=4)\n",
    "kmeans_8 = kmeans_segmentation(img, k=8)\n",
    "\n",
    "show_multiple([img, kmeans_2, kmeans_4, kmeans_8],\n",
    "              ['Original', 'K=2', 'K=4', 'K=8'], cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4477dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contour detection\n",
    "contours = contour_detection(gray)\n",
    "contour_img = draw_contours(img, contours)\n",
    "print(f\"Found {len(contours)} contours\")\n",
    "\n",
    "show_comparison(img, contour_img, ['Original', 'Contours Detected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb50620",
   "metadata": {},
   "source": [
    "## 5. Morphological Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e08992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary image for morphological operations\n",
    "_, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "eroded = erosion(binary, kernel_size=5)\n",
    "dilated = dilation(binary, kernel_size=5)\n",
    "opened = opening(binary, kernel_size=5)\n",
    "closed = closing(binary, kernel_size=5)\n",
    "\n",
    "show_multiple([binary, eroded, dilated, opened, closed],\n",
    "              ['Binary', 'Erosion', 'Dilation', 'Opening', 'Closing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4037b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced morphological operations\n",
    "gradient = morphological_gradient(binary)\n",
    "tophat = top_hat(gray)\n",
    "blackhat = black_hat(gray)\n",
    "skeleton = skeletonization(binary)\n",
    "boundary = boundary_extraction(binary)\n",
    "\n",
    "show_multiple([binary, gradient, tophat, blackhat, skeleton, boundary],\n",
    "              ['Binary', 'Gradient', 'Top Hat', 'Black Hat', 'Skeleton', 'Boundary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd394b6",
   "metadata": {},
   "source": [
    "## 6. Frequency Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a85ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFT and magnitude spectrum\n",
    "dft = compute_dft(gray)\n",
    "magnitude = get_magnitude_spectrum(dft)\n",
    "\n",
    "show_comparison(gray, magnitude, ['Original', 'Magnitude Spectrum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low-pass filters comparison\n",
    "ideal_lp = ideal_lowpass_filter(gray, cutoff=30)\n",
    "butter_lp = butterworth_lowpass_filter(gray, cutoff=30)\n",
    "gauss_lp = gaussian_lowpass_filter(gray, cutoff=30)\n",
    "\n",
    "show_multiple([gray, ideal_lp, butter_lp, gauss_lp],\n",
    "              ['Original', 'Ideal LP (D0=30)', 'Butterworth LP', 'Gaussian LP'], cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437a9f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-pass filter\n",
    "ideal_hp = ideal_highpass_filter(gray, cutoff=30)\n",
    "\n",
    "show_comparison(gray, ideal_hp, ['Original', 'Ideal High-Pass (D0=30)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c1745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homomorphic filter for illumination correction\n",
    "homomorphic = homomorphic_filter(gray, gamma_l=0.3, gamma_h=2.0, cutoff=30)\n",
    "\n",
    "show_comparison(gray, homomorphic, ['Original', 'Homomorphic Filter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d5cc01",
   "metadata": {},
   "source": [
    "## 7. Feature Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b8a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corner detection\n",
    "harris = detect_harris_corners(img)\n",
    "shi_tomasi, corners = detect_shi_tomasi_corners(img, max_corners=100)\n",
    "\n",
    "show_multiple([img, harris, shi_tomasi],\n",
    "              ['Original', 'Harris Corners', 'Shi-Tomasi Corners'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b998c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORB features\n",
    "orb_img, keypoints, descriptors = detect_orb_features(img, n_features=500)\n",
    "print(f\"Detected {len(keypoints)} ORB keypoints\")\n",
    "\n",
    "show_comparison(img, orb_img, ['Original', 'ORB Features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a769a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line and circle detection\n",
    "lines_img, lines = detect_lines_houghp(img)\n",
    "circles_img, circles = detect_circles_hough(img)\n",
    "\n",
    "show_multiple([img, lines_img, circles_img],\n",
    "              ['Original', 'Hough Lines', 'Hough Circles'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d505451c",
   "metadata": {},
   "source": [
    "## 8. Deep Learning / Artistic Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face detection (if faces present in image)\n",
    "face_img, faces = detect_faces_haar(img)\n",
    "print(f\"Detected {len(faces)} faces\")\n",
    "\n",
    "show_comparison(img, face_img, ['Original', 'Face Detection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artistic effects\n",
    "pencil = neural_style_transfer_simple(img, style='pencil')\n",
    "cartoon = neural_style_transfer_simple(img, style='cartoon')\n",
    "watercolor = neural_style_transfer_simple(img, style='watercolor')\n",
    "oil = neural_style_transfer_simple(img, style='oil')\n",
    "\n",
    "show_multiple([img, pencil, cartoon, watercolor, oil],\n",
    "              ['Original', 'Pencil Sketch', 'Cartoon', 'Watercolor', 'Oil Painting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157dc144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stylization and detail enhancement\n",
    "styled = stylization(img)\n",
    "detailed = detail_enhance(img)\n",
    "gray_sketch, color_sketch = pencil_sketch(img)\n",
    "\n",
    "show_multiple([img, styled, detailed, color_sketch],\n",
    "              ['Original', 'Stylized', 'Detail Enhanced', 'Color Pencil Sketch'], cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea6886b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Basic Operations**: Resize, rotate, flip, brightness, contrast, gamma, histogram equalization\n",
    "2. **Filters**: Average, Gaussian, median, bilateral, sharpen, emboss, denoise\n",
    "3. **Edge Detection**: Sobel, Canny, Laplacian, Prewitt, Roberts, Scharr\n",
    "4. **Segmentation**: Thresholding (simple, Otsu, adaptive), K-means, contours\n",
    "5. **Morphology**: Erosion, dilation, opening, closing, gradient, skeleton\n",
    "6. **Frequency Domain**: DFT, low/high-pass filters, homomorphic filter\n",
    "7. **Feature Detection**: Harris, Shi-Tomasi, ORB, Hough transforms\n",
    "8. **Deep Learning**: Face detection, artistic effects\n",
    "\n",
    "Use the Streamlit app (`streamlit run app.py`) for an interactive experience!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
